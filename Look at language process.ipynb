{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f7e46-73d8-483f-8d8b-01d29f1031c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "from transformers import BertModel, BertTokenizer, BertForMaskedLM\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "\n",
    "import faiss\n",
    "import scipy\n",
    "from scipy.sparse import coo_array\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import scipy.sparse as sparse\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ced1e-2a69-4f1a-99f4-a5517edb0e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf10712-cd61-45ec-bbcf-a4f536fe6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings and get knn\n",
    "\n",
    "# model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "# embeds = model.cls.predictions.decoder.weight#model.embeddings.word_embeddings.weight\n",
    "embeds = BertModel.from_pretrained(\"bert-base-uncased\").embeddings.word_embeddings.weight\n",
    "embeds = embeds.detach().cpu().numpy()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "vocab = np.array(list(tokenizer.get_vocab().keys()))\n",
    "\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "# lm_head = GPT2LMHeadModel.from_pretrained('gpt2').lm_head\n",
    "# embeds = lm_head.weight.detach().cpu().numpy()\n",
    "# vocab = np.array(list(tokenizer.get_vocab().keys()))\n",
    "\n",
    "norms = np.linalg.norm(embeds, axis=1, keepdims=True)\n",
    "embeds_normalized = embeds / norms#np.maximum(1, norms)\n",
    "\n",
    "unused = np.array(['[unused' in key for key in vocab])\n",
    "print(\"Constructing nearest neighbor matrix...\")\n",
    "\n",
    "english_alphabet = [\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "    'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
    "    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'\n",
    "]\n",
    "\n",
    "k = 20\n",
    "strong_masking = False\n",
    "is_unused = ((strong_masking) * np.array([t.startswith(\"[\") and t.endswith(\"]\") for t in vocab])).astype(bool)\n",
    "is_suffix = ((strong_masking) * np.array([t.startswith(\"##\") for t in vocab])).astype(bool)\n",
    "not_english = ((strong_masking) * (~np.array([all([x in english_alphabet for x in t]) for t in vocab]))).astype(bool)\n",
    "is_number = (np.array([any([x in np.arange(10).astype(str) for x in t]) for t in vocab])).astype(bool)\n",
    "is_normal = ~np.any([is_unused, is_suffix, is_number, not_english], axis=0)\n",
    "masking = ([(is_number, is_number+is_normal), (is_normal, is_normal)] if not strong_masking\n",
    "    else [(not_english, not_english), (is_unused, is_unused), (is_suffix, is_suffix), (is_number, is_number), (is_normal, is_normal)])\n",
    "indices = np.empty([len(embeds), k])\n",
    "distances = np.empty([len(embeds), k])\n",
    "range_ = np.arange(len(embeds))\n",
    "# for mask in tqdm([is_unused, is_suffix, is_number, is_normal]):\n",
    "#     search_mask = mask\n",
    "#     if mask.sum()>0:\n",
    "#         index = faiss.IndexFlatIP(embeds.shape[1]) \n",
    "#         index.add(embeds_normalized[search_mask])\n",
    "#         distances_temp, indices_temp = index.search(embeds_normalized[mask], k+1)\n",
    "#         distances[mask] = distances_temp[:, 1:]\n",
    "#         indices[mask] = range_[search_mask][indices_temp[:, 1:]]\n",
    "for mask, search_mask in tqdm(masking):\n",
    "    if mask.sum()>0:\n",
    "        index = faiss.IndexFlatIP(embeds.shape[1]) \n",
    "        index.add(embeds_normalized[search_mask])\n",
    "        print(\"added\")\n",
    "        distances_temp, indices_temp = index.search(embeds_normalized[mask], k+1)\n",
    "        distances[mask] = distances_temp[:, 1:]\n",
    "        indices[mask] = range_[search_mask][indices_temp[:, 1:]]\n",
    "print([m.sum() for m in [is_unused, is_suffix, is_number, is_normal]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381fb9c9-95d7-4ea9-a1fc-65de33e247a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c681931a-9ca3-4324-9247-8dc114671e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ = 1 # max_ = 2.1\n",
    "# plot knn\n",
    "plt.figure(figsize=[4, 4])\n",
    "_, bins, _ = plt.hist(distances[:, 0][~unused], bins=100, color='red', alpha=0.5, label='closest');\n",
    "_, bins, _ = plt.hist(distances[:, -1][~unused], bins=100, color='blue', alpha=0.5, label='furthest');\n",
    "plt.hist(distances[:, -1][unused], bins=bins, color='black', label='unused tokens');\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.xlabel(\"neighbour similarity\")\n",
    "plt.xlim(0, max_**2)\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=[6, 2])\n",
    "plt.plot(distances.min(-1), color='black')\n",
    "plt.xlabel(\"token number\")\n",
    "plt.ylabel(\"min neighbour sim\")\n",
    "plt.ylim(0, max_**2)\n",
    "\n",
    "plt.figure(figsize=[6, 2])\n",
    "plt.plot(norms, color='black')\n",
    "plt.xlabel(\"token number\")\n",
    "plt.ylabel(\"min neighbour sim\")\n",
    "# plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffed265-514d-4c87-8844-afc4b4f93097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples\n",
    "inds = np.random.randint(len(embeds), size=5)\n",
    "for ind in inds:\n",
    "    print(tokenizer.decode([ind]), \":\", ' '.join(vocab[indices[ind].astype(int)]))\n",
    "    print(distances[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dbabb6-26c1-4cb5-af52-2ff6ddb5b973",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_sym = False\n",
    "bandwidth = 0.2 / np.log(2)\n",
    "normalize = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24605e80-dc06-4b52-a8da-f69b83d42d8f",
   "metadata": {},
   "source": [
    "Say $K$ is a symmetric stochastic matrix, the vector of all ones is an eigenvector, so the generator for the uniform process commutes with $K$.\n",
    "If $\\mathcal L$ is the generator with $-1$ diagonal and we add $w\\mathcal L$ to $K$ then we subtract $w\\frac N {N-1}$ from all but the top eval.\n",
    "Since $K$ is degenerate, this ends up being the eigenvalue gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b7d81f-81b7-4a60-b8e3-411ee505f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proc(gamma, normalize, make_sym, bandwidth, k):\n",
    "    row_indices = np.repeat(np.arange(embeds.shape[0]), k) \n",
    "    col_indices = indices[:, :k].flatten()\n",
    "    dot_products = distances[:, :k].flatten()\n",
    "    # rates = distances.sum(-1)\n",
    "    assert (dot_products > 0).all()\n",
    "    assert (row_indices != col_indices).all()\n",
    "    if make_sym:\n",
    "        row_indices, col_indices = np.r_[row_indices, col_indices], np.r_[col_indices, row_indices]\n",
    "        dot_products = np.r_[dot_products, dot_products]\n",
    "    dot_products = np.exp((1 - dot_products) / bandwidth)\n",
    "    \n",
    "    sparse_matrix = coo_array((dot_products, (row_indices, col_indices)), shape=(embeds.shape[0], embeds.shape[0]))\n",
    "    sparse_matrix.sum_duplicates()\n",
    "    sparse_matrix_csr = sparse_matrix.tocsr()\n",
    "    L_diag = - sparse_matrix_csr.sum(-1)\n",
    "    L_off_diag = sparse_matrix_csr\n",
    "    if normalize:\n",
    "        L_off_diag = coo_array(L_off_diag / (-L_diag)[:, None])\n",
    "        L_diag = -1 + 0 * L_diag\n",
    "    rate = - (L_diag.min()) / (1-gamma) \n",
    "    L = L_off_diag / rate + scipy.sparse.diags(L_diag / rate)\n",
    "    K = L_off_diag / rate + scipy.sparse.diags(L_diag / rate + 1)\n",
    "    L = L.tocoo()\n",
    "    K = K.tocoo()\n",
    "    \n",
    "    K_gpu = torch.sparse_coo_tensor((K.row, K.col), K.data, size=(embeds.shape[0], embeds.shape[0])).float().cuda()\n",
    "    K_gpu = K_gpu.to_sparse_csr()\n",
    "    L_gpu = torch.sparse_coo_tensor((L.row, L.col), L.data, size=(embeds.shape[0], embeds.shape[0])).float().cuda()\n",
    "    L_gpu = L_gpu.to_sparse_csr()\n",
    "    \n",
    "    L = L.tocsr()\n",
    "    K = K.tocsr()\n",
    "    \n",
    "    K_coo = K_gpu.to_sparse_coo()\n",
    "    K_T = K_coo.transpose(0, 1).coalesce().to_sparse_csr()\n",
    "    return L, K, K_gpu, L_gpu, K_coo, K_T\n",
    "\n",
    "L, K, K_gpu, L_gpu, K_coo, K_T = get_proc(gamma, normalize, make_sym, bandwidth, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bffeb19-2312-44f5-b7c3-5d2cce0a3485",
   "metadata": {},
   "outputs": [],
   "source": [
    "stationary = scipy.sparse.linalg.eigs(K.T, 1, which='LR')[1][:, 0]\n",
    "stationary = torch.tensor(stationary).float().cuda()\n",
    "    assert torch.isclose((stationary**2).sum(), torch.ones_like((stationary**2).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac07e8d-fae4-4605-86fb-eec0affe664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69421dd0-18c0-4ffa-92e7-bc84dc0a4002",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(-L.diagonal()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574aa328-2e0f-496f-afa8-791b4055bc54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a8275b-00e1-43a9-ad59-53bb6fdbd6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb9527-b17f-42ff-b705-dc0f043cf73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = torch.tensor(np.ones([1, K.shape[0]]), device='cuda').float()\n",
    "pbar = tqdm(range(30000))\n",
    "for i in pbar:\n",
    "    x_0_new = x_0 + (L_T @ x_0.T).T + 0.001 * (x_0.mean(1) - x_0)\n",
    "    err = torch.sqrt(((x_0_new- x_0) ** 2).sum())\n",
    "    if torch.allclose(err, torch.zeros_like(err)):\n",
    "        break\n",
    "    x_0 = x_0_new\n",
    "    if i%1000 == 0:\n",
    "        pbar.set_description(f\"err:{err.item()}\")\n",
    "plt.plot(x_0.cpu().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2767201-c255-4970-a09e-ca14539112ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try mult\n",
    "x_0 = torch.tensor(np.random.randn(16, 1024, K.shape[0]), device='cuda').float()\n",
    "S = torch.tensor(np.random.randint(300, size=x_0.shape[:2]), device='cuda').long()\n",
    "\n",
    "def K_power_mult(S, x_0, period=1):\n",
    "    shape = x_0.shape\n",
    "    x_0 = x_0.reshape(-1, x_0.shape[-1]).T\n",
    "    curr_liks = x_0\n",
    "    liks = torch.ones_like(x_0)\n",
    "    curr_S = S.reshape(-1)\n",
    "    pbar = tqdm(total=curr_S.max().item(), unit=\"iteration\",\n",
    "                position=0, leave=True)\n",
    "    while torch.any(curr_S > 0):\n",
    "        active = curr_S >= 0\n",
    "        liks[:, curr_S == 0] = curr_liks[:, (curr_S == 0)[active]]\n",
    "        if curr_liks.shape[-1] == 1:\n",
    "            if not all((curr_S > 0)[active]):\n",
    "                break\n",
    "        else:\n",
    "            curr_liks = curr_liks[:, (curr_S > 0)[active]]\n",
    "        probs = K_gpu @ curr_liks\n",
    "        # x_curr = sample_probs(probs)\n",
    "        curr_S = curr_S - 1\n",
    "        pbar.update(1)\n",
    "    if curr_liks.shape[-1] > 0:\n",
    "        liks[:, curr_S == 0] = curr_liks\n",
    "    return liks.T.reshape(shape)\n",
    "\n",
    "start_time = time.time()\n",
    "K_power_mult(S, x_0)\n",
    "torch.cuda.synchronize()\n",
    "time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a484140-50e2-4336-b35b-d6d00d0ed110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# try mi\n",
    "mat_all = torch.eye(K.shape[0]).float().cuda()\n",
    "p0 = torch.randn(K.shape[0]).float().cuda() ** 2\n",
    "p0 = p0 / p0.sum()\n",
    "ent_p0 = -torch.xlogy(p0, p0).sum()\n",
    "\n",
    "batch_size = 2000\n",
    "mis = torch.ones(K.shape[0])\n",
    "for j in tqdm(range(math.ceil(K.shape[0] / batch_size))):\n",
    "    mat = mat_all[:, j*batch_size:(j+1)*batch_size]\n",
    "    for i in range(1000):\n",
    "        # p = p0[:, None] * mat\n",
    "        # p = torch.where(p < 0, 0, p)\n",
    "        # p_sum = p.sum(0)\n",
    "        # mi = (torch.xlogy(p, p).sum() - torch.xlogy(p_sum, p_sum).sum()) / ent_p0\n",
    "        # mis[i] = mis[i] + mi\n",
    "    \n",
    "        # stat_part = stationary @ mat\n",
    "        # diff = mat - stat_part * stationary[:, None]\n",
    "        mat = K_T @ mat\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b1d4ed-f5c5-4ef8-8577-75db60463304",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(mis).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ff59c2-60ca-47f7-b049-4c489a33a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(K_gpu.values().sum())\n",
    "print((K_gpu @ torch.ones(K.shape[0]).cuda()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1518bc4-98b7-422c-a048-92b6b35edf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_gpu.to_sparse_csr().float()@ torch.ones([K.shape[0], 1]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d018f8-3cf5-4cb0-adec-d7f693d29dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l)\n",
    "print(u.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d934dc-3ade-4e17-b75e-58ad486177b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = torch.tensor(np.random.randint(len(embeds), size=[16, 1024]), device='cuda').long()\n",
    "S = torch.tensor(np.random.randint(1000, size=x_0.shape), device='cuda').long()\n",
    "\n",
    "def sample_probs(p):\n",
    "    r, c = p.shape\n",
    "    p[:, 0] = 0\n",
    "    data_cs = p.data.cumsum()\n",
    "    rows_cs = np.r_[[0], data_cs[p.indptr[1:] - 1]]\n",
    "    data_cs = ((data_cs - np.repeat(rows_cs[:-1], np.diff(p.indptr)))\n",
    "               / np.repeat(rows_cs[1:], np.diff(p.indptr))\n",
    "               - np.repeat(np.random.rand(r)/rows_cs[1:], np.diff(p.indptr)))\n",
    "    return p.indices[np.r_[[False], np.diff((data_cs >= 0).astype(int)) > 0]]\n",
    "\n",
    "def f(S, x_0, period=1):\n",
    "    shape = x_0.shape\n",
    "    x_0 = x_0.flatten().cpu().numpy()\n",
    "    x_curr = x_0\n",
    "    x_t = np.ones_like(x_0)\n",
    "    curr_S = S.flatten().cpu().numpy()\n",
    "    pbar = tqdm(total=curr_S.max(), unit=\"iteration\",\n",
    "                position=0, leave=True)\n",
    "    while any(curr_S > 0):\n",
    "        active = curr_S >= 0\n",
    "        x_t[curr_S == 0] = x_curr[(curr_S == 0)[active]]\n",
    "        if len(x_curr) == 1:\n",
    "            if not all((curr_S > 0)[active]):\n",
    "                break\n",
    "        else:\n",
    "            x_curr = x_curr[(curr_S > 0)[active]]\n",
    "        probs = K[x_curr]\n",
    "        # print(x_curr.dtype, sample_probs(probs).dtype)\n",
    "        x_curr = sample_probs(probs)\n",
    "        curr_S = curr_S - 1\n",
    "        pbar.update(1)\n",
    "    if len(x_curr) > 0:\n",
    "        x_t[curr_S == 0] = x_curr\n",
    "    return x_t.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1244bd8-7d22-4a27-ac26-1c389d1ffe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = f(torch.arange(2000).cuda(), 2000 * torch.ones(2000).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dcc84e-2db8-4ec2-9bae-757ffd159950",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t#, x_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70f8c87-160a-4efe-b193-1609deb230e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = L.to_dense()\n",
    "print(A.dtype, A.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e725650-2fb8-4155-bc94-cff083f30ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.randn([len(embeds), 10], device=device)\n",
    "L.to_dense() @ g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e48a67-1af7-4c69-b8e2-63c398ea9755",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.randn([len(embeds), 10], device=device)\n",
    "%timeit L @ g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac740f-1c6d-44d6-acaf-a9a09a0a5dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(L.toarray()[2000:2100, 2000:2100], vmin=-1, vmax=1, cmap='bwr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814cb646-da01-48b5-9766-0a27fc597129",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.linalg.eigs(K, k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028fed18-454e-44ca-9d0c-482d03103767",
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.random.randint(len(embeds), size=(32, 1024))\n",
    "%timeit K[inds.ravel(), :].toarray().reshape(*inds.shape, K.shape[1])\n",
    "%timeit K.T[inds.ravel(), :].toarray().reshape(*inds.shape, K.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5744b76-7ccd-46b4-bbb0-47985c848527",
   "metadata": {},
   "source": [
    "#### look at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c7f59-2c07-4f14-bbf6-e1699495cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import data\n",
    "cfg = OmegaConf.load('configs/basic_language.yaml')\n",
    "train_dataloader, test_dataloader = data.get_dataloaders(cfg)\n",
    "\n",
    "datum = next(iter(train_dataloader))\n",
    "[tokenizer.decode(t) for t in datum['input_ids'][0].reshape(-1, 128)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dada2b26-26c3-48f0-bc34-6479e6fa5461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c00e7c-a595-46bb-9c95-0b5455a2a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "normalize = True\n",
    "make_sym = True\n",
    "bandwidth = 0.3\n",
    "k = 20\n",
    "*_, K_coo, __ = get_proc(gamma, normalize, make_sym, bandwidth, k)\n",
    "K_coo = K_coo.cpu()\n",
    "N = K_T.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b0067-9c1e-48a2-9530-63cdc04f7dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "up = 0.0\n",
    "\n",
    "x = datum['input_ids'][0]\n",
    "for i in range(1000):\n",
    "    k_proc = K_coo.index_select(0, x).to_dense()\n",
    "    x = (1-up) * k_proc + (up / N) * (1-gamma)\n",
    "    x = torch.multinomial(x, num_samples=1, replacement=True).squeeze(-1)\n",
    "    clear_output(wait=True)\n",
    "    print(i * (1-gamma), '\\n', tokenizer.decode(x[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b22ec-bb25-4488-aa17-e91dc0262a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7bfa35-1553-43f9-bf44-e2d0e3601623",
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.random.randint(len(embeds), size=(128, 1000))\n",
    "%timeit (K@K[:, inds.ravel()]).toarray().reshape(K.shape[0], *inds.shape)\n",
    "%timeit K@(K[:, inds.ravel()].toarray()).reshape(K.shape[0], *inds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa64a7-b76b-4b22-a25d-e24be7981dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_powers = 20\n",
    "current_prod = scipy.sparse.eye(K.shape[0])\n",
    "K_powers = [current_prod]\n",
    "for _ in range(num_powers):\n",
    "    current_prod = current_prod @ K\n",
    "    K_powers.append(current_prod)\n",
    "for i in range(num_powers):\n",
    "    scipy_coo = K_powers[i].tocoo()\n",
    "    row = torch.from_numpy(scipy_coo.row.astype(np.int64))\n",
    "    col = torch.from_numpy(scipy_coo.col.astype(np.int64))\n",
    "    data = torch.from_numpy(scipy_coo.data)\n",
    "    indices = torch.stack([row, col], dim=0)\n",
    "    shape = scipy_coo.shape\n",
    "    torch_sparse_tensor = torch.sparse_coo_tensor(indices, data, size=shape)\n",
    "    K_powers[i] = torch_sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d7eba-1c7f-4efc-abcf-e1f135e5d97d",
   "metadata": {},
   "source": [
    "### other code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f253b5ef-ab77-4cd4-9573-7adc3609cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class all_ones(cola.ops.operator_base.LinearOperator):\n",
    "    def _matmat(self, v):\n",
    "        return v.sum(0, keepdim=True)\n",
    "\n",
    "dtype = torch.float32\n",
    "device = 'cpu'\n",
    "N = len(embeds)\n",
    "# weight = torch.tensor(1/20000, dtype=dtype, device=device)\n",
    "# L = cola.ops.Sparse(torch.tensor(dot_products).to(dtype).to(device),\n",
    "#                     torch.tensor(row_indices).to(dtype).to(device),\n",
    "#                     torch.tensor(col_indices).to(dtype).to(device),\n",
    "#                     shape=(N, N)) \n",
    "# L = cola.ops.Dense(L.to_dense())\n",
    "# ones = all_ones(dtype, (N, N))\n",
    "# ones.device = L.device\n",
    "# L = L #+ weight * (ones - N * cola.ops.I_like(L))\n",
    "# rate = (torch.tensor(rates / rates.max(), dtype=dtype, device=device) + (N-1) * weight).max() / (1-gamma)\n",
    "# K = L / rate + cola.ops.I_like(L)\n",
    "\n",
    "l, u = cola.linalg.eig(K, 1)\n",
    "l2, u2 = cola.linalg.eig(K - cola.ops.Dense(u)@cola.ops.Dense(u.T), 1)\n",
    "print(l, l2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evodiff",
   "language": "python",
   "name": "evodiff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
