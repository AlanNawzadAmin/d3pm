{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef5ef31-444c-4091-88a1-f5b59d6e492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "from d3pm_sc.d3pm import D3PM\n",
    "from d3pm_sc.d3pm_classic import D3PM_classic\n",
    "from d3pm_sc.unet import UNet, SimpleUNet\n",
    "from d3pm_sc.dit import DiT_Llama\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e6afe-cdfd-420c-b9b7-a69c45656461",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 128  # number of classes for discretized state per pixel\n",
    "n_T = 1000\n",
    "n_channel = 3\n",
    "gamma = 0\n",
    "\n",
    "s_dim = 4\n",
    "batch_size = 16\n",
    "\n",
    "# # Masking\n",
    "# d3pm = D3PM_classic(DiT_Llama(n_channel, N+1, n_T, False, s_dim=s_dim, dim=1024),\n",
    "#             n_T, num_classes=N+1, hybrid_loss_coeff=0.01, forward_type='masking').cuda()\n",
    "# masking = True\n",
    "# # Uniform\n",
    "# d3pm = D3PM_classic(UNet(n_channel, N, n_T, False),\n",
    "#             n_T, num_classes=N, hybrid_loss_coeff=0.01).cuda()\n",
    "# masking = False\n",
    "# Schedule conditioning\n",
    "d3pm = D3PM(SimpleUNet(n_channel, N, n_T, True, s_dim=4),\n",
    "            n_T, num_classes=N, hybrid_loss_coeff=0.01, gamma=gamma).cuda()\n",
    "masking = False\n",
    "print(f\"Total Param Count: {sum([p.numel() for p in d3pm.x0_model.parameters()])}\")\n",
    "dataset = CIFAR10(\n",
    "    \"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "train_size = int(len(dataset) * 0.9)\n",
    "dataset, test_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdcb556-feb8-4a80-be82-61a98f5c691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=[6, 3])\n",
    "ax[0].semilogy(d3pm.beta_t, label=\"Hazard\", color='black')\n",
    "ax[0].legend()\n",
    "\n",
    "steps = torch.arange(d3pm.n_T + 1, dtype=torch.float64) / d3pm.n_T\n",
    "alpha_bar = 1-torch.cos((1 - steps) * torch.pi / 2)\n",
    "ax[1].plot(alpha_bar, label=\"p(unmut)\", color='black')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bda4255-2115-4acf-bf0e-ad866cfcebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "optim = torch.optim.AdamW(d3pm.x0_model.parameters(), lr=1e-3)\n",
    "d3pm.train()\n",
    "\n",
    "n_epoch = 14\n",
    "device = \"cuda\"\n",
    "\n",
    "global_step = 0\n",
    "for i in range(n_epoch):\n",
    "\n",
    "    pbar = tqdm(dataloader)\n",
    "    loss_ema = None\n",
    "    for x, cond in pbar:\n",
    "        optim.zero_grad()\n",
    "        x = x.to(device)\n",
    "        cond = cond.to(device)\n",
    "        x = (x * (N - 1)).round().long().clamp(0, N - 1)\n",
    "        \n",
    "        loss, info = d3pm(x, cond)\n",
    "\n",
    "        loss.backward()\n",
    "        norm = torch.nn.utils.clip_grad_norm_(d3pm.x0_model.parameters(), 0.1)\n",
    "\n",
    "        # log\n",
    "        with torch.no_grad():\n",
    "            param_norm = sum([torch.norm(p) for p in d3pm.x0_model.parameters()])\n",
    "\n",
    "        if loss_ema is None:\n",
    "            loss_ema = loss.item()\n",
    "        else:\n",
    "            loss_ema = 0.99 * loss_ema + 0.01 * loss.item()\n",
    "        pbar.set_description(\n",
    "            f\"loss: {loss_ema:.4f}, norm: {norm:.4f}, param_norm: {param_norm:.4f}, vb_loss: {info['vb_loss']:.4f}, ce_loss: {info['ce_loss']:.4f}\"\n",
    "        )\n",
    "        optim.step()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 300 == 1:\n",
    "            d3pm.eval()\n",
    "            with torch.no_grad():\n",
    "                # save images\n",
    "                cond = torch.arange(0, 4).cuda() % 10\n",
    "                init_noise = torch.randint(0, N, (4,)+x.shape[1:]).cuda()\n",
    "                if masking:\n",
    "                    init_noise = N + 0*init_noise\n",
    "\n",
    "                images = d3pm.sample_with_image_sequence(\n",
    "                    init_noise, cond, stride=40\n",
    "                )\n",
    "                # image sequences to gif\n",
    "                gif = []\n",
    "                for image in images:\n",
    "                    x_as_image = make_grid(image.float() / (N - 1), nrow=2)\n",
    "                    img = x_as_image.permute(1, 2, 0).cpu().numpy()\n",
    "                    img = (img * 255).astype(np.uint8)\n",
    "                    gif.append(Image.fromarray(img))\n",
    "\n",
    "                gif[0].save(\n",
    "                    f\"contents/sample_{global_step}.gif\",\n",
    "                    save_all=True,\n",
    "                    append_images=gif[1:],\n",
    "                    duration=100,\n",
    "                    loop=0,\n",
    "                )\n",
    "\n",
    "                last_img = gif[-1]\n",
    "                last_img.save(f\"contents/sample_{global_step}_last.png\")\n",
    "\n",
    "                val_loss = []\n",
    "                for x, cond in test_dataloader:\n",
    "                    optim.zero_grad()\n",
    "                    x = x.to(device)\n",
    "                    cond = cond.to(device)\n",
    "                    x = (x * (N - 1)).round().long().clamp(0, N - 1)\n",
    "                    \n",
    "                    loss, info = d3pm(x, cond)\n",
    "                    val_loss.append(info['vb_loss'])\n",
    "                bpd = np.mean(val_loss).round(4)\n",
    "                print(\"val bits per dim:\", bpd, \", perplexity:\", np.exp(bpd).round(4))\n",
    "\n",
    "            d3pm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f74a8-103d-4673-b0d1-3c687ce0695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(d3pm.state_dict(), 'models/d3pm_cifar10_aug10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5f341-b727-4e5f-a078-238538d89388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "n_samples = 20\n",
    "\n",
    "cond = torch.zeros(n_samples).cuda().to(torch.int64)\n",
    "# init_noise = torch.randint(0, N, (n_samples, n_channel, 32, 32)).cuda() # uniform\n",
    "init_noise = N + 0*torch.randint(0, N, (n_samples, n_channel, 32, 32)).cuda() # masking\n",
    "\n",
    "images = d3pm.sample_with_image_sequence(\n",
    "    init_noise, cond, stride=1\n",
    ")\n",
    "images = torch.cat([im[None, ...] for im in images], 0)\n",
    "images = torch.cat([init_noise[None, ...], images], 0)\n",
    "diffs = (images[1:] != images[:-1]).reshape(n_T-1, n_samples, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc54e48-fe3b-4e53-8314-75ca2ad5d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fracs = (images == N).reshape(n_T, n_samples, -1).float().cpu().mean(-1).mean(-1)\n",
    "plt.figure(figsize=[3, 3])\n",
    "plt.plot(fracs, color='black')\n",
    "plt.ylim(0, 1.02)\n",
    "plt.xlim(0, n_T)\n",
    "plt.ylabel(\"fraction masked\")\n",
    "plt.xlabel(\"Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd068f-eab6-49b9-9236-fbfc77ca9e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = diffs.cpu().numpy().mean(-1).mean(-1)[::-1]\n",
    "r2 = d3pm.beta_t[1:].cpu().numpy()# * (1-gamma)\n",
    "plt.figure(figsize=[5, 5])\n",
    "plt.semilogy(r1, label=\"Empirical\", color='black')\n",
    "plt.semilogy(r2, label=\"Analytic\", color='blue')\n",
    "plt.xlabel(\"time step\")\n",
    "plt.ylabel(\"p(mut)\")\n",
    "plt.legend()\n",
    "# plt.savefig(\"figures/schedule_comparison.png\")\n",
    "\n",
    "plt.figure(figsize=[5, 5])\n",
    "plt.semilogy(r1[:10], label=\"Empirical\", color='black')\n",
    "plt.semilogy(r2[:10], label=\"Analytic\", color='blue')\n",
    "plt.xlabel(\"time step\")\n",
    "plt.ylabel(\"p(mut)\")\n",
    "plt.legend()\n",
    "# plt.savefig(\"figures/schedule_comparison_zoom.png\")\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogy(r1/r2, color='black')\n",
    "plt.plot([0, 1000], np.ones(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ddb14-0d3d-4688-9d60-a8935ec4f652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evodiff",
   "language": "python",
   "name": "evodiff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
